{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "     -------------------------------------- 250.0/250.0 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.03 MiB for an array with shape (300, 300, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7332\\30020743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mimg_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mimg_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# / 255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtrain_x_im\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.03 MiB for an array with shape (300, 300, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Self_Driving_CNN_model.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1q9taOjxGJxIKqvotSXIo1tXOu90g7acB\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join, isfile\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install openpyxl\n",
    "\n",
    "# Change directory to the data directory from hw1\n",
    "#os.chdir(r\"C:\\Users\\blume\\Desktop\\School\\Self Driving Cars\\Project\\train_images\")\n",
    "count = 0\n",
    "train_x_im = []\n",
    "parent_fldr = \"D:/CMU/CMU Classes/Self Driving Cars/Project/Perception/Data/train_images\"\n",
    "dir_list = os.listdir(r\"D:/CMU/CMU Classes/Self Driving Cars/Project/Perception/Data/train_images\")\n",
    "\n",
    "test_fldr = \"D:/CMU/CMU Classes/Self Driving Cars/Project/Perception/Data/test_images\"\n",
    "test_list = os.listdir(r\"D:/CMU/CMU Classes/Self Driving Cars/Project/Perception/Data/test_images\")\n",
    "\n",
    "# Get labels for all images from given excel file\n",
    "labels = pd.read_excel(r\"D:/CMU/CMU Classes/Self Driving Cars/Project/Perception/Data/trainval/labels.xlsx\")\n",
    "train_labels = []\n",
    "label_count = 0\n",
    "\n",
    "# Open all the images from the files list for both train and test data and append to a large tensor\n",
    "# Normalize the data so it is easier to process\n",
    "for file in dir_list:\n",
    "    curr_label = labels.iat[label_count,1]\n",
    "    label_count = label_count + 1\n",
    "    train_labels.append(curr_label)\n",
    "\n",
    "    # Process the image to make it standard for the NN\n",
    "    full_file = parent_fldr + \"/\" + file\n",
    "    with Image.open(full_file) as img:\n",
    "        img = img.resize((300, 300))\n",
    "        img_data = np.asarray(img)\n",
    "        img_data = img_data.astype(np.float32) # / 255.0\n",
    "        img_tensor = torch.from_numpy(img_data).float()\n",
    "        train_x_im.append(img_tensor)\n",
    "\n",
    "train_x = torch.stack(train_x_im)\n",
    "print(np.shape(train_x))\n",
    "print(np.shape(train_labels))\n",
    "# Rearange tensor\n",
    "#train_shuf, labels_shuf = shuffle(train_x, train_labels, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_im = []\n",
    "# Test data tensor\n",
    "for image in test_list:\n",
    "    # Process the image to make it standard for the NN\n",
    "    full_file = test_fldr + \"/\" + image\n",
    "    with Image.open(full_file) as img:\n",
    "        img = img.resize((300, 300))\n",
    "        img_data = np.asarray(img)\n",
    "        img_data = img_data.astype(np.float32) # / 255.0\n",
    "        img_tensor = torch.from_numpy(img_data).float()\n",
    "        test_true_im.append(img_tensor)\n",
    "\n",
    "test_true = torch.stack(test_true_im)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(train_x, train_labels, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5262, 3, 300, 300])\n",
      "torch.Size([5073, 3, 300, 300])\n",
      "torch.Size([2500, 3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#X_train = X_train.unsqueeze(0)\n",
    "#X_test = X_test.unsqueeze(0)\n",
    "#X_train = X_train.permute(0, 3, 1, 2)\n",
    "#X_test = X_test.permute(0, 3, 1, 2)\n",
    "#test_x_im = test_x_im.permute(0,2,1,3)\n",
    "print(np.shape(test_x_im))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "\n",
    "# Create a tensor for the labels as well\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "train_label = torch.tensor(y_train).long()\n",
    "test_label = torch.tensor(y_test).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = test_true.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "# Create CNN model with various layers. Apply pooling and RELU\n",
    "# Make sure to set the linear layers correctly\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3,20,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4,4),\n",
    "            nn.Conv2d(64,64,7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4,4),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            #nn.Linear(3136,6000),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(3136,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = Conv2D()\n",
    "\n",
    "# Create your train and test datasets with labels\n",
    "batch_size = 200\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(X_train,train_label),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(X_test,test_label),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_true_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(test_x_im),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test = np.zeros([2631,])\n",
    "true_test = np.array(true_test)\n",
    "true_label = torch.tensor(true_test).long()\n",
    "test_true_loader = torch.utils.data.DataLoader(dataset=torch.utils.data.TensorDataset(test_true,true_label),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, train loss: 2.0956, train acc: 0.5425, test loss: 0.8050, test acc: 0.6348\n",
      "Epoch 2/7, train loss: 0.8007, train acc: 0.6318, test loss: 0.8301, test acc: 0.6348\n",
      "Epoch 3/7, train loss: 0.8049, train acc: 0.6322, test loss: 0.7835, test acc: 0.6348\n",
      "Epoch 4/7, train loss: 0.7797, train acc: 0.6328, test loss: 0.7976, test acc: 0.6128\n",
      "Epoch 5/7, train loss: 0.7814, train acc: 0.6320, test loss: 0.7740, test acc: 0.6324\n",
      "Epoch 6/7, train loss: 0.7629, train acc: 0.6391, test loss: 0.7645, test acc: 0.6324\n",
      "Epoch 7/7, train loss: 0.7483, train acc: 0.6412, test loss: 0.7564, test acc: 0.6340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 7\n",
    "train_loss_tot = []\n",
    "train_acc_tot = []\n",
    "test_loss_tot = []\n",
    "test_acc_tot = []\n",
    " \n",
    "# Loop through the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    " \n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    # iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        #inputs = inputs.reshape(batch_size,1,300,300)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # increment the running loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.argmax(1) == labels).sum().item()\n",
    " \n",
    "    # calculate the average training loss and accuracy\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_tot.append(train_loss)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_acc_tot.append(train_acc)\n",
    " \n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Validation data\n",
    "        for inputs, labels in test_loader:\n",
    "            #inputs = inputs.reshape(batch_size,1,300,300) \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        \n",
    " \n",
    "    # calculate the average validation loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_loss_tot.append(test_loss)\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "    test_acc_tot.append(test_acc)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, test loss: {test_loss:.4f}, test acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m predictions \u001b[39m=\u001b[39m outputs_true\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mguid/image\u001b[39m\u001b[39m'\u001b[39m: test_list, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: predictions}\n\u001b[1;32m---> 11\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data\u001b[39m=\u001b[39;49md)\n",
      "File \u001b[1;32mc:\\Users\\blume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\blume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\blume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\blume\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Given test data set \n",
    "outputs_true_all = []\n",
    "with torch.no_grad():\n",
    "    for inputs, true_label in test_true_loader:\n",
    "        #inputs = inputs.reshape(batch_size,1,300,300) \n",
    "        outputs_true = model(inputs)\n",
    "        maxes = outputs_true[0].argmax(1)\n",
    "        outputs_true_all.append(maxes)\n",
    "\n",
    "all_maxes = np.array(all_maxes)\n",
    "predictions = outputs_true_all.argmax(1)\n",
    "d = {'guid/image': test_list, 'label': predictions}\n",
    "df = pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions\u001b[39m.\u001b[39mshape\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[39m=\u001b[39m outputs_true_all\u001b[39m.\u001b[39;49margmax(\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mguid/image\u001b[39m\u001b[39m'\u001b[39m: test_list, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: predictions}\n\u001b[0;32m      4\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39md)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "predictions.shape\n",
    "predictions = outputs_true_all.argmax(1)\n",
    "d = {'guid/image': test_list, 'label': predictions}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2,\n",
       "       1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2,\n",
       "       2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1,\n",
       "       2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0,\n",
       "       2, 1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_true_all[0].shape\n",
    "outputs_true_all[0].argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(outputs_true)\n",
    "maxes = arr.argmax(1)\n",
    "print(maxes)\n",
    "all_maxes = []\n",
    "all_maxes.append(maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]\n",
      " [2 1 2 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 2 2 1 1]]\n",
      "(9, 31)\n",
      "(279, 1)\n"
     ]
    }
   ],
   "source": [
    "#all_maxes.append(maxes)\n",
    "print(all_maxes)\n",
    "all_maxes = np.array(all_maxes)\n",
    "print(all_maxes.shape)\n",
    "new_maxes = np.reshape(all_maxes,(-1,1))\n",
    "print(new_maxes.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
